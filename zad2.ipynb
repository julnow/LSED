{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasyfikacja zbioru win :-) Dane znajduja się pod https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data, natomiast opis do nich jest https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names. Należy:\n",
    "\n",
    "1. wczytać dane,\n",
    "2. nazwać kolumny (korzystając z opisu),\n",
    "3. sprawdzić jakie są parametry klasyfikatorów LDA, QDA i NB na pełnym zbiorze danych (wszystkie składowe obserwacji),\n",
    "4. ograniczyc się do 2 pierwszych, 5 pierwszych i 10 pierwszych składowych i sprawdzić skuteczności klasyfikatorów,\n",
    "5. ograniczyć się do 2 pierwszych zmiennych, podzielić zbiór (PU/PW/PT) 50/25/25 i w ten sposób dokonać wyboru spośród LDA, QDA, NB.\n",
    "6. ograniczyć się do 2 pierwszych zmiennych, wykonać kroswalidację w przypadku LDA, porównac z poprzednim punktem oraz powtórnym podstawieniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "    'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity',\n",
    "    'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "\n",
    "wines = pd.read_csv('wine.data', header=None, names=['Classification'] + column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classification  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0               1    14.23        1.71  2.43               15.6        127   \n",
       "1               1    13.20        1.78  2.14               11.2        100   \n",
       "2               1    13.16        2.36  2.67               18.6        101   \n",
       "3               1    14.37        1.95  2.50               16.8        113   \n",
       "4               1    13.24        2.59  2.87               21.0        118   \n",
       "5               1    14.20        1.76  2.45               15.2        112   \n",
       "6               1    14.39        1.87  2.45               14.6         96   \n",
       "7               1    14.06        2.15  2.61               17.6        121   \n",
       "8               1    14.83        1.64  2.17               14.0         97   \n",
       "9               1    13.86        1.35  2.27               16.0         98   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "5           3.27        3.39                  0.34             1.97   \n",
       "6           2.50        2.52                  0.30             1.98   \n",
       "7           2.60        2.51                  0.31             1.25   \n",
       "8           2.80        2.98                  0.29             1.98   \n",
       "9           2.98        3.15                  0.22             1.85   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  \n",
       "5             6.75  1.05                          2.85     1450  \n",
       "6             5.25  1.02                          3.58     1290  \n",
       "7             5.05  1.06                          3.58     1295  \n",
       "8             5.20  1.08                          2.85     1045  \n",
       "9             7.22  1.01                          3.55     1045  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trening klasyfikatorów na wybranych parametrów\n",
    "def train_classifier(x_df: pd.DataFrame, y_df: pd.DataFrame, train_columns: List[str]):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    nb = GaussianNB()\n",
    "\n",
    "    for clf in [lda, qda, nb]:\n",
    "        clf.fit(x_df[train_columns], y_df)\n",
    "    return lda, qda, nb\n",
    "\n",
    "def test_classifier(x_df: pd.DataFrame, y_df: pd.DataFrame, train_columns: List[str], lda, qda, nb):\n",
    "\n",
    "    preds = pd.DataFrame(np.array([clf.predict(x_df[train_columns]) for clf in [lda, qda, nb]]).T)\n",
    "    preds.columns = [\"lda\", \"qda\", \"nb\"]  \n",
    "    print(f\"Dla pierwszych {len(train_columns)} parametrów\")\n",
    "    for i, col in enumerate(preds.columns):\n",
    "        print(col)\n",
    "        print(metrics.classification_report(y_df, preds[col]))\n",
    "\n",
    "def train_test_classifier(x_df: pd.DataFrame, y_df: pd.DataFrame, train_columns: List[str]):\n",
    "    lda, qda, nb = train_classifier(x_df, y_df, train_columns)\n",
    "    test_classifier(x_df, y_df, train_columns, lda, qda, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla pierwszych 13 parametrów\n",
      "lda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        59\n",
      "           2       1.00      1.00      1.00        71\n",
      "           3       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           1.00       178\n",
      "   macro avg       1.00      1.00      1.00       178\n",
      "weighted avg       1.00      1.00      1.00       178\n",
      "\n",
      "qda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      1.00      0.99        59\n",
      "           2       1.00      0.99      0.99        71\n",
      "           3       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           0.99       178\n",
      "   macro avg       0.99      1.00      0.99       178\n",
      "weighted avg       0.99      0.99      0.99       178\n",
      "\n",
      "nb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.98      0.99        59\n",
      "           2       0.99      0.99      0.99        71\n",
      "           3       0.98      1.00      0.99        48\n",
      "\n",
      "    accuracy                           0.99       178\n",
      "   macro avg       0.99      0.99      0.99       178\n",
      "weighted avg       0.99      0.99      0.99       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dla wszystkich parametrów\n",
    "train_test_classifier(wines, wines.Classification, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla pierwszych 10 parametrów\n",
      "lda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      1.00      0.99        59\n",
      "           2       0.99      0.99      0.99        71\n",
      "           3       1.00      0.98      0.99        48\n",
      "\n",
      "    accuracy                           0.99       178\n",
      "   macro avg       0.99      0.99      0.99       178\n",
      "weighted avg       0.99      0.99      0.99       178\n",
      "\n",
      "qda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      1.00      0.99        59\n",
      "           2       1.00      0.99      0.99        71\n",
      "           3       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           0.99       178\n",
      "   macro avg       0.99      1.00      0.99       178\n",
      "weighted avg       0.99      0.99      0.99       178\n",
      "\n",
      "nb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.95      0.96        59\n",
      "           2       0.94      0.96      0.95        71\n",
      "           3       0.98      0.98      0.98        48\n",
      "\n",
      "    accuracy                           0.96       178\n",
      "   macro avg       0.96      0.96      0.96       178\n",
      "weighted avg       0.96      0.96      0.96       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dla pierwszych 10 parametrów\n",
    "train_test_classifier(wines, wines.Classification, column_names[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla pierwszych 5 parametrów\n",
      "lda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.92      0.92        59\n",
      "           2       0.88      0.92      0.90        71\n",
      "           3       0.80      0.77      0.79        48\n",
      "\n",
      "    accuracy                           0.88       178\n",
      "   macro avg       0.87      0.87      0.87       178\n",
      "weighted avg       0.88      0.88      0.88       178\n",
      "\n",
      "qda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.90      0.92        59\n",
      "           2       0.90      0.92      0.91        71\n",
      "           3       0.80      0.83      0.82        48\n",
      "\n",
      "    accuracy                           0.89       178\n",
      "   macro avg       0.88      0.88      0.88       178\n",
      "weighted avg       0.89      0.89      0.89       178\n",
      "\n",
      "nb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.88      0.90        59\n",
      "           2       0.86      0.87      0.87        71\n",
      "           3       0.78      0.79      0.78        48\n",
      "\n",
      "    accuracy                           0.85       178\n",
      "   macro avg       0.85      0.85      0.85       178\n",
      "weighted avg       0.85      0.85      0.85       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dla pierwszych 5 parametrów\n",
    "train_test_classifier(wines, wines.Classification, column_names[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla pierwszych 2 parametrów\n",
      "lda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.86      0.84        59\n",
      "           2       0.86      0.86      0.86        71\n",
      "           3       0.73      0.67      0.70        48\n",
      "\n",
      "    accuracy                           0.81       178\n",
      "   macro avg       0.80      0.80      0.80       178\n",
      "weighted avg       0.81      0.81      0.81       178\n",
      "\n",
      "qda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.88      0.85        59\n",
      "           2       0.85      0.87      0.86        71\n",
      "           3       0.74      0.65      0.69        48\n",
      "\n",
      "    accuracy                           0.81       178\n",
      "   macro avg       0.80      0.80      0.80       178\n",
      "weighted avg       0.81      0.81      0.81       178\n",
      "\n",
      "nb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.86      0.84        59\n",
      "           2       0.85      0.87      0.86        71\n",
      "           3       0.72      0.65      0.68        48\n",
      "\n",
      "    accuracy                           0.81       178\n",
      "   macro avg       0.80      0.79      0.80       178\n",
      "weighted avg       0.81      0.81      0.81       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dla pierwszych 2 parametrów\n",
    "train_test_classifier(wines, wines.Classification, column_names[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#podział na PU, PW, PT dla 2 pierwszych parametrów\n",
    "x_train, x_test, y_train, y_test = train_test_split(wines[column_names[0:2]], wines.Classification, test_size=0.5, stratify = wines.Classification)\n",
    "x_test, x_validation, y_test, y_validation = train_test_split(x_test, y_test, test_size=0.5, stratify = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla pierwszych 2 parametrów\n",
      "lda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.93      0.78        15\n",
      "           2       0.81      0.76      0.79        17\n",
      "           3       0.86      0.50      0.63        12\n",
      "\n",
      "    accuracy                           0.75        44\n",
      "   macro avg       0.78      0.73      0.73        44\n",
      "weighted avg       0.77      0.75      0.74        44\n",
      "\n",
      "qda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      1.00      0.81        15\n",
      "           2       0.81      0.76      0.79        17\n",
      "           3       1.00      0.50      0.67        12\n",
      "\n",
      "    accuracy                           0.77        44\n",
      "   macro avg       0.83      0.75      0.76        44\n",
      "weighted avg       0.82      0.77      0.76        44\n",
      "\n",
      "nb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      1.00      0.81        15\n",
      "           2       0.81      0.76      0.79        17\n",
      "           3       1.00      0.50      0.67        12\n",
      "\n",
      "    accuracy                           0.77        44\n",
      "   macro avg       0.83      0.75      0.76        44\n",
      "weighted avg       0.82      0.77      0.76        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#trening na pierwszych 2 parametrach, na .5 danych\n",
    "lda, qda, nb = train_classifier(x_train, y_train, column_names[0:2])\n",
    "#test na zbiorze testowym\n",
    "test_classifier(x_test, y_test, column_names[0:2], lda, qda, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla pierwszych 2 parametrów\n",
      "lda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.93      0.87        15\n",
      "           2       0.89      0.94      0.92        18\n",
      "           3       0.89      0.67      0.76        12\n",
      "\n",
      "    accuracy                           0.87        45\n",
      "   macro avg       0.87      0.85      0.85        45\n",
      "weighted avg       0.87      0.87      0.86        45\n",
      "\n",
      "qda\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.73      0.79        15\n",
      "           2       0.86      1.00      0.92        18\n",
      "           3       0.64      0.58      0.61        12\n",
      "\n",
      "    accuracy                           0.80        45\n",
      "   macro avg       0.78      0.77      0.77        45\n",
      "weighted avg       0.79      0.80      0.79        45\n",
      "\n",
      "nb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.73      0.79        15\n",
      "           2       0.90      1.00      0.95        18\n",
      "           3       0.67      0.67      0.67        12\n",
      "\n",
      "    accuracy                           0.82        45\n",
      "   macro avg       0.80      0.80      0.80        45\n",
      "weighted avg       0.82      0.82      0.82        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#walidacja\n",
    "test_classifier(x_validation, y_validation, column_names[0:2], lda, qda, nb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie zbioru testowego i walidacyjnego nie mozemy stwierdzic, czy lepsza skutecznosc ma model lda, czy qda (w pierwszej probie lepszy lda, w drugiej qda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ErrorbarContainer object of 3 artists>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#kroswalidacja dla lda\n",
    "\n",
    "def test_crossvalidation(clf, wines, m, n):\n",
    "    kf = RepeatedKFold(n_repeats = m, n_splits = n)\n",
    "    score = cross_val_score(clf, wines[column_names[0:2]], wines.Classification, cv = kf)\n",
    "    return score.mean(), score.std()\n",
    "    \n",
    "z = np.array([test_crossvalidation(lda, wines, 5, i) for i in range(2, 40)])\n",
    "\n",
    "plt.errorbar(range(2,40), z[:,0], yerr = z[:,1], fmt=\"o\", capsize=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po skorzystaniu z kroswalidacji otrzymujemy mniejszą precyzję, niz w przypadku ponownego podstawienia (co wiaze sie jednak z overfittingiem dla tego podejscia), sredni blad latwiej okreslic niz w przypadku poprzedniego punktu, gdzie losowo testowaliśmi dwa razy po 0.25 zbioru wejściowego. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbm22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
